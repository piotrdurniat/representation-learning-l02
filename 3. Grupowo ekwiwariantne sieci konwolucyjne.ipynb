{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3994c60a",
   "metadata": {},
   "source": [
    "Przed oddaniem zadania upewnij się, że wszystko działa poprawnie.\n",
    "**Uruchom ponownie kernel** (z paska menu: Kernel$\\rightarrow$Restart) a następnie\n",
    "**wykonaj wszystkie komórki** (z paska menu: Cell$\\rightarrow$Run All).\n",
    "\n",
    "Upewnij się, że wypełniłeś wszystkie pola `TU WPISZ KOD` lub `TU WPISZ ODPOWIEDŹ`, oraz\n",
    "że podałeś swoje imię i nazwisko poniżej:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1163276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Piotr Durniat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c561b5",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84234e55",
   "metadata": {},
   "source": [
    "# 3. Grupowo ekwiwariantne sieci konwolucyjne\n",
    "\n",
    "Ostatnią architekturą, którą dzisiaj sprawdzimy są tzw. _Group Equivariant Neural Networks_, czyli sieci neuronowe (w naszym przypadku konwolucyjne), które opisują funkcje ekwiwariantne względem określonej grupy symetrii. Po raz pierwszy koncepcja takich sieci została opisana w 2016 r. przez Cohena i Wellinga w pracy [Group Equivariant Convolutional Networks](https://arxiv.org/pdf/1602.07576.pdf). W pracy formalizują zastosowanie grup symetrii do modelowania struktury danych – dokładniej mówiąc rozważają dwie grupy: _p4_ (obroty o 90 stopni) oraz _p4m_ (obroty o 90 stopni i odbicia).\n",
    "\n",
    "Implementacja grupowo ekwiwariantnych sieci jest zadaniem nietrywialnym i czasochłonnym, stąd w tym zeszycie wykorzystamy bibliotekę `e2cnn`, która posiada zaimplementowane warstwy grupowo ekwiwariantne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030701f",
   "metadata": {},
   "source": [
    "## 3.1. Architektura\n",
    "\n",
    "Założeniem modelu będzie, aby koder był w stanie stworzyć niezmienniczą (ang. _invariant_) reprezentację podstawowych i obróconych obrazów wejściowych. Moduł dekodera nie będziemy modyfikować i użyjemy klasy `CNNDecoder` z poprzedniego zeszytu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb388707",
   "metadata": {},
   "source": [
    "## Zadanie 3.1. (4 pkt)\n",
    "\n",
    "W celu zrozumienia grupowo ekwiwariantnych sieci zaleca się zapoznanie z poniższymi materiałami:\n",
    "\n",
    "- [2-częściowy blog na temat G-CNN](https://fabianfuchsml.github.io/equivariance1of2/)\n",
    "- [Wykład Erika Bekkersa \"Group Equivariant Deep Learning (UvA - 2022)\" (Lecture 1.1-1.7)](https://youtube.com/playlist?list=PL8FnQMH2k7jzPrxqdYufoiYVHim8PyZWd)\n",
    "\n",
    "Zapoznaj się z biblioteką E2CNN:\n",
    "\n",
    "- [Przykład użycia](https://github.com/QUVA-Lab/e2cnn#getting-started)\n",
    "- [Zeszyt wprowadzający](https://github.com/QUVA-Lab/e2cnn/blob/master/examples/introduction.ipynb)\n",
    "\n",
    "i zaimplementuj moduł kodera z następującą architekturą:\n",
    "\n",
    "- dla każdej warstwy konwolucji przymij takie same parametry jak dla `CNNEncoder` z poprzedniego zeszytu, tzn. `kernel_size=3`, `stride=2`, `padding=1`\n",
    "- pierwsza warstwa konwolucji obrotowej `R2Conv` obliczająca 8 filtrów wyjściowych\n",
    "- aktywacja ReLU\n",
    "- druga warstwa konwolucji obrotowej `R2Conv` obliczająca 16 filtrów wyjściowych\n",
    "- aktywacja ReLU\n",
    "- trzecia warstwa konwolucji obrotowej `R2Conv` obliczająca 32 filtry wyjściowe\n",
    "- redukacja maksymalizująca (`PointwiseMaxPoolAntialiased`) z kernelem o wymiarze `3`\n",
    "- redukcja grupowa (`GroupPooling`)\n",
    "- część wielowarstwowego perceptrona z modelu `CNNEncoder`, tzn.:\n",
    "  - spłaszczenie macierzy do wektora,\n",
    "  - warstwa liniowa z 128 cechami wyjściowymi\n",
    "  - aktywacja ReLU\n",
    "  - warstwa liniowa z `latent_dim` cechami wyjściowymi\n",
    "\n",
    "Jako grupę symetrii przyjmij grupę obrotów dyskretnych, tzw. grupę cykliczną $C_N$ (`Rot2dOnR2`).\n",
    "\n",
    "Pamiętaj użyciu właściwych modułów z pakietu `e2cnn.nn` oraz `torch.nn`.\n",
    "**Pamiętaj, że opórcz zamplementowania modelu należy dobrze zapoznać się z teorią. Warto również zweryfikować, jakie są i do czego odnoszą się wymiary tensorów zwracanych przez każdą warstwę.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446ad1d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e37697159a4a13db628a38dd3c81023d",
     "grade": true,
     "grade_id": "g-cnn-encoder-implementation",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from e2cnn import gspaces\n",
    "from e2cnn import nn\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class GCNNEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim: int, N: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, img: torch.Tensor) -> torch.Tensor:\n",
    "        # TU WPISZ KOD\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a3531",
   "metadata": {},
   "source": [
    "## 3.2. Uczenie modelu\n",
    "\n",
    "W przeciwieństwie do poprzednich zeszytów, nie załączono wcześniej przygotowanego modelu, a uczenie należy przeprowadzić samemu. Uczenie powinno zająć kilkanaście/kilkadziesiąt minut, a w razie mocno ograniczonych zasobów należy skorzystać z Google Colab (aby wykorzystać GPU wystarczy przekazać odpowiedni parametr do `Trainer`, co jest opisane w dokumentacji: [Trainer API](https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-class-api), [użycie GPU](https://lightning.ai/docs/pytorch/stable/accelerators/gpu_basic.html)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = \"./data/GCNN/logs\"\n",
    "CHECKPOINT_PATH = \"./data/GCNN/model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7770c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $LOG_PATH --host 0.0.0.0 --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e737db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import SampledMNISTData\n",
    "from src.ae import Autoencoder\n",
    "from src.cnn import CNNDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNAutoencoder(Autoencoder):\n",
    "\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super().__init__(\n",
    "            encoder=GCNNEncoder(latent_dim=latent_dim, N=8),\n",
    "            decoder=CNNDecoder(latent_dim=latent_dim),\n",
    "        )\n",
    "        self.save_hyperparameters({\"latent_dim\": latent_dim})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e4e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from src.dataset import SampledMNISTData\n",
    "\n",
    "\n",
    "if os.path.isfile(CHECKPOINT_PATH):\n",
    "    # if already trained, load saved checkpoint\n",
    "    # it is normal that some parameters are missing (difference is due to eval and train modes)\n",
    "    gcnn_ae = GCNNAutoencoder.load_from_checkpoint(\n",
    "        CHECKPOINT_PATH, strict=False, map_location=\"cpu\"\n",
    "    )\n",
    "else:\n",
    "    # train model otherwise\n",
    "    gcnn_ae = GCNNAutoencoder(latent_dim=2)\n",
    "    trainer = Trainer(\n",
    "        max_epochs=10,\n",
    "        logger=TensorBoardLogger(\n",
    "            save_dir=LOG_PATH, name=\"gcnn_ae\", default_hp_metric=False\n",
    "        ),\n",
    "        callbacks=[],\n",
    "        enable_checkpointing=False,\n",
    "        accelerator=\"auto\",\n",
    "    )\n",
    "    mnist = SampledMNISTData(num_samples_per_class=-1)\n",
    "\n",
    "    trainer.fit(\n",
    "        model=gcnn_ae,\n",
    "        train_dataloaders=mnist,\n",
    "    )\n",
    "\n",
    "    trainer.save_checkpoint(filepath=CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b5af7",
   "metadata": {},
   "source": [
    "## 3.3. Badanie jakości reprezentacji w zadaniu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce8c9a",
   "metadata": {},
   "source": [
    "Wczytujemy losową próbkę zbioru danych MNIST:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b780bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = SampledMNISTData(num_samples_per_class=100, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116ecc1",
   "metadata": {},
   "source": [
    "Dla każdej próbki wyciągamy jej reprezentację (wektor dwu-wymiarowy) oraz etykietę:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d59298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import extract_representations\n",
    "\n",
    "\n",
    "representations = extract_representations(\n",
    "    model=gcnn_ae,\n",
    "    dataset=mnist,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import evaluate_linear\n",
    "\n",
    "\n",
    "auc_train, auc_test = evaluate_linear(\n",
    "    z_train=representations[\"train\"][\"z\"],\n",
    "    y_train=representations[\"train\"][\"y\"],\n",
    "    z_test=representations[\"test\"][\"z\"],\n",
    "    y_test=representations[\"test\"][\"y\"],\n",
    ")\n",
    "\n",
    "print(f\"AUC => train: {auc_train * 100.0:.2f} [%], test: {auc_test * 100.0:.2f} [%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be327d5b",
   "metadata": {},
   "source": [
    "## 3.4. Wizualizacja przestrzeni reprezentacji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae906f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from src.utils import visualize_latent_spaces\n",
    "\n",
    "\n",
    "visualize_latent_spaces(representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b379074f",
   "metadata": {},
   "source": [
    "## 3.5. Wizualizacja jakości rekonstrukcji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566854ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from src.utils import visualize_random_sample\n",
    "\n",
    "    \n",
    "visualize_random_sample(model=gcnn_ae, dataset=mnist.mnist_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee4ea3",
   "metadata": {},
   "source": [
    "## 3.6. Odporność na obroty\n",
    "\n",
    "Zbadamy jak się zachowuje model oparty o grupowo ekwiwariantne sieci konwolucyjne w przypadku obrotów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cc03b",
   "metadata": {},
   "source": [
    "Wybieramy dowolną instancję ze zbioru danych:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233a0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = mnist.mnist_train[256]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924350a6",
   "metadata": {},
   "source": [
    "## Zadanie 3.3. (0 pkt)\n",
    "\n",
    "Skopiuj implementację funkcji `rotate` z poprzedniego zeszytu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119edff4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84c173e51b1753d06b8399b752be597b",
     "grade": true,
     "grade_id": "rotate-copy",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def rotate(img: torch.Tensor, angle: float) -> torch.Tensor:\n",
    "    # TU WPISZ KOD\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d194a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from src.transforms import plot_transformation\n",
    "\n",
    "\n",
    "plot_transformation(\n",
    "    image=img,\n",
    "    model=gcnn_ae,\n",
    "    transformation_fn=rotate,\n",
    "    min_param_value=0,\n",
    "    max_param_value=360,\n",
    "    step=45,\n",
    "    keep_channel_dim=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd767aa",
   "metadata": {},
   "source": [
    "## Zadanie 3.4 (1 pkt)\n",
    "\n",
    "Czy autokoder G-CNN jest odporny na obroty obiektów? Z czego może to wynikać?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb423c73",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55de0bc1f0a1b1447bfaad4fe9d3a89a",
     "grade": true,
     "grade_id": "gcnn-rotate",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "TU WPISZ ODPOWIEDŹ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
